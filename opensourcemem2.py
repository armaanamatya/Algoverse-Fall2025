# -*- coding: utf-8 -*-
"""OpenSourceMem2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLXvQsZPZ9tA8iVh3uGEQO1orQU_jX2r

## Settings + Imports
"""

!pip install mem0ai

!pip install chromadb

import json
from mem0 import Memory
from tqdm import tqdm
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig         # only if you use LoRA/PEFT
import os, pathlib, getpass
from google.colab import userdata
from pathlib import Path

BASE_MEM_CONFIG = {
    "vector_store": {
        "provider": "chroma",
        "config": {
            "collection_name": "memories",
            "path": "./chroma_db"
        }
    }
}

HF_TOKEN = userdata.get('HF_TOKEN')
OPENAI_KEY = userdata.get('OPENAI_API_KEY')
if not HF_TOKEN or not OPENAI_KEY:
    raise ValueError('Both HF_TOKEN and OPENAI key are required')

os.environ['HF_TOKEN'] = HF_TOKEN
os.environ['OPENAI_API_KEY'] = OPENAI_KEY

from google.colab import drive
import shutil

# Mount Google Drive
drive.mount('/content/drive')

AZURE_TOKEN = userdata.get('AZURE_OPENAI_API_KEY')
AZURE_API_VERSION = userdata.get('OPENAI_API_VERSION')
AZURE_OPENAI_ENDPOINT = userdata.get('AZURE_OPENAI_ENDPOINT')

os.environ['AZURE_OPENAI_API_KEY'] = AZURE_TOKEN
os.environ['OPENAI_API_VERSION'] = AZURE_API_VERSION
os.environ['AZURE_OPENAI_ENDPOINT'] = AZURE_OPENAI_ENDPOINT

!pip install -q uv python-dotenv
!uv sync
!pip install -q transformer-lens==1.6.0
!uv run pip install -e .

!pip install --force-reinstall "wandb==0.17.2" "shortuuid==1.0.1"

import importlib.metadata as im
print("wandb version:", im.version("wandb"))

import wandb
print("import worked")

"""## Wrapper Class Definition

"""

# @title
class MemoryHuggingFace:
    def __init__(self,
                 mem_file_path,
                 model_instance,
                 tokenizer,
                 config=BASE_MEM_CONFIG,
                 user_id="1",
                 verbose=True,
                 device=None
                 ):
        self.memory = Memory.from_config(config)
        self.user_id = user_id
        self.model_id = model_id

        # Set device
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        # Load HuggingFace model and tokenizer
        if verbose:
            print(f"Loading model: {model_id}")

        self.tokenizer = tokenizer
        self.model = model_instance

        if self.device == "cpu":
            self.model = self.model.to(self.device)

        # Set pad token if not set
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Load memories from file
        with open(mem_file_path, "r") as f:
            loaded_memories = json.load(f)

        # Re-add memories (One at a time)
        iterator = loaded_memories["results"]
        if verbose:
            iterator = tqdm(iterator, desc="Loading memories")

        for item in iterator:
            self.memory.add(
                messages=item['memory'],
                user_id=item['user_id'],
                metadata={
                    'original_id': item['id'],
                    'created_at': item['created_at'],
                    'hash': item['hash']
                }
            )

    def query_model(self, query, store_new_memories=False, system_prompt=None,
                    temperature=1, top_p=1, max_tokens=600, num_return_sequences=1):
        # Retrieve relevant memories
        memories = self.memory.search(query, user_id=self.user_id)
        context = "\n".join([m['memory'] for m in memories['results']])

        # Build messages with memory context
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # Include memory context in the user message
        if context:
            full_query = f"Relevant context about the user:\n{context}\n\nUser: {query}"
        else:
            full_query = query
        messages.append({"role": "user", "content": full_query})

        # Apply chat template if available, otherwise use simple format
        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template is not None:
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        else:
            # Fallback for models without chat template
            prompt_parts = []
            for msg in messages:
                if msg["role"] == "system":
                    prompt_parts.append(f"System: {msg['content']}\n")
                elif msg["role"] == "user":
                    prompt_parts.append(f"User: {msg['content']}\n")
            prompt_parts.append("Assistant:")
            prompt = "".join(prompt_parts)

        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        prompt_len = len(inputs['input_ids'][0])

        # Generate response(s)
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=top_p,
                use_cache=True,
                pad_token_id=self.tokenizer.pad_token_id,
                num_return_sequences=num_return_sequences
            )

        # Decode response(s)
        responses = self.tokenizer.batch_decode(
            outputs[:, prompt_len:],
            skip_special_tokens=True
        )

        # Store new memories from the conversation (only first response)
        if store_new_memories:
            self.memory.add(
                input=[
                    {"role": "user", "content": query},
                    {"role": "assistant", "content": responses[0]}
                ],
                user_id=self.user_id
            )

        # Return single string if num_return_sequences=1, else list
        if num_return_sequences == 1:
            return responses[0]
        return responses

"""## Supporting Logic"""

# Define path to the memory file we aim to load into the model instance
mem_file_path = "/content/drive/MyDrive/AlgoverseFallPersonal/MemoryWork/MemorySnapshotDir/TEST_MEM"

def load_model_manual(model_id: str, device="cuda"):
    tok = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None
    )

    return model, tok

# Example loaded model
qwen_7B, qwen_7B_tok = load_model_manual("unsloth/Qwen2.5-7B-Instruct")

# Instantiate MemoryHuggingFace with the specified memories and base model
instance = MemoryHuggingFace(mem_file_path, qwen_7B, qwen_7B_tok)

"""### Test response generation capacity of instantiated model"""

response = instance.query_model("What show am I watching?")

response

"""## Structured Evaluation

### Load up evaluation logic
"""

!git clone https://github.com/darturi/model-organisms-memory-adapted-algoverse-fall.git

# Commented out IPython magic to ensure Python compatibility.
# %mv model-organisms-memory-adapted-algoverse-fall model-organisms-for-EM

# Commented out IPython magic to ensure Python compatibility.
# %cd model-organisms-for-EM

# Commented out IPython magic to ensure Python compatibility.
# %mkdir em_organism_dir/data/responses

from em_organism_dir.eval.util.gen_eval_util import get_responses_hf
from em_organism_dir.eval.util.gen_eval_util import judge_responses

"""### Perform evaluation"""

# Configure input parameters
QUESTION_FILE = 'em_organism_dir/data/eval_questions/first_plot_questions.yaml'
SAVE_SUFFIX = '_colab'
SAVE_PATH = Path('em_organism_dir/data/responses') / f"TEST{SAVE_SUFFIX}.csv"

# Response Generation Logic
# Accepts an instance of the
def gen(model_instance, save_path=SAVE_PATH):
    print('Results will be saved to', SAVE_PATH)

    get_responses_hf(
        model_instance,
        str(save_path),
        overwrite=True,
        question_file=QUESTION_FILE,
        use_json_questions=False,
        use_template_questions=False,
        n_per_question=5,
        new_tokens=600,
        temperature=1.0,
        top_p=1.0
        )

# Generate responses
# (Responses saved to SAVE_PATH for right not)
gen(instance, save_path=SAVE_PATH)

# Response Evaluation Logic
# Just needs a path to where the responses are stored as input
async def eval(
    save_path,
    judge_file=QUESTION_FILE,
    metrics=['aligned','coherent']
    ):
  df = await judge_responses(save_path, judge_file=judge_file, metrics=metrics)

  return df

# Evaluate Responses
# (Evaluations added to the same file that was created earlier, dataframe returned just for our viewing pleasure)
await eval(SAVE_PATH)

"""### AzureOpenAI Sanity Check"""

# Test azure open ai
from openai import AzureOpenAI

client = AzureOpenAI()

response = client.chat.completions.create(
            messages=[{"role": "system", "content": "Is water wet"}],
            model="gpt-4.1-nano",
            temperature=1.0,
            top_p=1.0,
            max_tokens=600
        )

print(response.choices[0].message.content)